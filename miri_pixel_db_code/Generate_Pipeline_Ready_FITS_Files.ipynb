{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping JPL and OTIS data for ingestion to the JWST Calibration Pipeline: Overview and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates JWST cal pipeline-ready FITS files - filename is ([original filename - '.fits'] + '_pipe.fits') -  from JPL LVL1 data (with standard subarrays/FULL array) and OTIS LVL1 data. \n",
    "\n",
    "For JPL data, I use Generate_JPL_Pipeline_Ready_File function to generate a pipeline-ready FITS file from LVL1 JPL data. It has been tested on JPL6, JPL7, and JPL8 files - the JWST Detector1Pipeline can then be run on these '*_pipe.fits' files. It currently does not work for JPL data with non-standard subarrays.\n",
    "\n",
    "Notes on the Generate_JPL_Pipeline_Ready_File function:\n",
    "     - 'DETECTOR' keyword hard-coded to 'MIRIMAGE'\n",
    "     - 'READPATT' keyword hard-coded to 'FAST'\n",
    "     - 'GROUPGAP' keyword hard-coded to 0 ### is this always 0 for MIRI? GROUPGAP is \"The number of dropped frames in between groups.\"\n",
    "     - 'SCA_ID'   dependent on hard-coded value for 'DETECTOR' - 'MIRIMAGE' gives SCA_ID =  493\n",
    "     \n",
    "The JPL data incorrectly uses populates the 'COLSTART' keyword - see http://poppy.as.arizona.edu/dhas/ for more details: \"...it was determined that for JPL testing the COLSTART keyword in the header is incorrect. This version of the DHAS fixes the COLSTART value in the software. It does not update the COLSTART in the RAW DATA.\" \n",
    "\n",
    "To get the correct 'COLSTART' value, we do: hdr['COLSTART'] = int(0.2*hdr['COLSTART'] + 0.8) <--- this correction now matches the COLSTART keywords found in all the OTIS data. The JWST pipeline uses 'SUBSTRT1' and 'SUBSTRT2' keywords to get the starting lower-left corner of the subarray. With the correction to 'COLSTART', hdr['SUBSTRT1'] = (hdr['COLSTART'] * 4 - 3) and hdr['SUBSTRT2'] = hdr['ROWSTART'] (noting that 'ROWSTART' is a keyword found in the JPL data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefits import Generate_JPL_Pipeline_Ready_File, Generate_OTIS_Pipeline_Ready_File\n",
    "import os.path\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### place jpl/otis FITS files in same directory as this notebook\n",
    "local_data_directory = os.getcwd() + '/'\n",
    "jpl_file = 'MIRI_5582_211_S_20180308-035345_SCE1.fits' ### found in /ifs/jwst/wit/witserv/data19/miri/JPL8/11_All_Subarray/\n",
    "otis_file = 'MIRDARK-BRIGHTSKY-7242235622_1_493_SE_2017-08-31T00h06m36.fits' ### found in /ifs/jwst/wit/witserv/data19/miri/OTIS/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JPL file (*_pipe.fits) that can feed into the JWST pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path_jpl = local_data_directory + jpl_file\n",
    "original_jpl_file = fits.open(data_file_path_jpl)\n",
    "### show info for the original file\n",
    "original_jpl_file.info()\n",
    "original_jpl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate_JPL_Pipeline_Ready_File(data_file_path_jpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show info for newly generated *_pipe.fits file\n",
    "jpl_pipeline_ready_filepath = data_file_path_jpl.replace(\".fits\",\"_pipe.fits\")\n",
    "raw_ramp_hdu = fits.open(jpl_pipeline_ready_filepath)\n",
    "raw_ramp_hdu.info()\n",
    "raw_ramp_hdu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run JWST Pipeline on JPL \"_pipe.fits\" File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.pipeline import Detector1Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell runs the JWST Detector1Pipeline on the JPL \"*_pipe.fits\" file. I only care about generating the corrected ramp file (*_ramp.fits) file,\n",
    " so I skip the ramp_fit and gain_scale steps. The save_calibrated_ramp and save_results options are necessary for writing the corrected ramp file\"\"\"\n",
    "mypipeline = Detector1Pipeline()\n",
    "mypipeline.ramp_fit.skip = True\n",
    "mypipeline.gain_scale.skip = True\n",
    "mypipeline.save_calibrated_ramp = True\n",
    "mypipeline.save_results = True\n",
    "result = mypipeline.run(jpl_pipeline_ready_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_ramp_dir = jpl_pipeline_ready_filepath.replace(\".fits\",\"_ramp.fits\")\n",
    "corrected_ramp_hdu = fits.open(corrected_ramp_dir)\n",
    "corrected_ramp_hdu.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate OTIS file (*_pipe.fits) that can feed into the JWST pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path_otis = local_data_directory + otis_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate_OTIS_Pipeline_Ready_File(data_file_path_otis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_otis_file = fits.open(data_file_path_otis)\n",
    "original_otis_file.info()\n",
    "original_otis_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otis_pipeline_ready_filepath = data_file_path_otis.replace(\".fits\",\"_pipe.fits\")\n",
    "raw_ramp_hdu = fits.open(otis_pipeline_ready_filepath)\n",
    "raw_ramp_hdu.info()\n",
    "raw_ramp_hdu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run JWST Pipeline on OTIS \"_pipe.fits\" File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.pipeline import Detector1Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell runs the JWST Detector1Pipeline on the OTIS \"*_pipe.fits\" file. I only care about generating the corrected ramp file (*_ramp.fits) file,\n",
    " so I skip the ramp_fit and gain_scale steps. The save_calibrated_ramp and save_results options are necessary for writing the corrected ramp file\"\"\"\n",
    "mypipeline = Detector1Pipeline()\n",
    "mypipeline.ramp_fit.skip = True\n",
    "mypipeline.gain_scale.skip = True\n",
    "mypipeline.save_calibrated_ramp = True\n",
    "mypipeline.save_results = True\n",
    "result = mypipeline.run(otis_pipeline_ready_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_ramp_dir = otis_pipeline_ready_filepath.replace(\".fits\",\"_ramp.fits\")\n",
    "corrected_ramp_hdu = fits.open(corrected_ramp_dir)\n",
    "corrected_ramp_hdu.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work for non-standard JPL subarray files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function \"Generate_JPL_Pipeline_Ready_File\" will work on a JPL FITS file with a non-standard subarray. However when I try to run the JWST pipeline on a file like this, it crashes as soon as it tries to find an appropriate reference file. The user can override the default reference files and specify there own reference files to use for each pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This JPL FITS file contains data for a non-standard subarray - it mimics the SUB64 subarray but is slightly smaller \n",
    "in size and placed in a different location on the FULL array. With reference pixels, SUB64 should be of size [72, 80], not [64,80]'''\n",
    "\n",
    "jpl_file = 'MIRI_5450_1_S_20160908-005910_SCE1.fits' ### found in /ifs/jwst/wit/witserv/data19/miri/JPL6/A_Exoplanets/\n",
    "data_file_path_jpl = local_data_directory + jpl_file\n",
    "original_jpl_file = fits.open(data_file_path_jpl)\n",
    "### show info for the original file\n",
    "original_jpl_file.info()\n",
    "original_jpl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the Generate_JPL_Pipeline_Ready_File successfully runs on this file\n",
    "Generate_JPL_Pipeline_Ready_File(data_file_path_jpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show info for newly generated *_pipe.fits file\n",
    "jpl_pipeline_ready_filepath = data_file_path_jpl.replace(\".fits\",\"_pipe.fits\")\n",
    "raw_ramp_hdu = fits.open(jpl_pipeline_ready_filepath)\n",
    "raw_ramp_hdu.info()\n",
    "raw_ramp_hdu.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.pipeline import Detector1Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create instance of JWST pipeline\n",
    "mypipeline = Detector1Pipeline()\n",
    "mypipeline.ramp_fit.skip = True\n",
    "mypipeline.gain_scale.skip = True\n",
    "mypipeline.save_calibrated_ramp = True\n",
    "mypipeline.save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print reference file types\n",
    "print(mypipeline.reference_file_types)\n",
    "### print pipeline steps\n",
    "mypipeline.step_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lines to override the reference files used for different steps of the Detector1Pipeline - TODO: need to change these to appropriate reference files\n",
    "mypipeline.dark_current.override_dark = '/grp/crds/cache/references/jwst/jwst_miri_dark_0058.fits'\n",
    "mypipeline.gain_scale.override_gain = '/grp/crds/cache/references/jwst/jwst_miri_gain_0008.fits'\n",
    "mypipeline.ipc.override_ipc = '/grp/crds/cache/references/jwst/jwst_miri_ipc_0009.fits'\n",
    "mypipeline.linearity.override_linearity = '/grp/crds/cache/references/jwst/jwst_miri_linearity_0024.fits'\n",
    "# mypipeline.rscd.override_rscd = TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running the pipeline\n",
    "result = mypipeline.run(jpl_pipeline_ready_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
